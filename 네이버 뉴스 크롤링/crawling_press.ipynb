{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8c10a50",
   "metadata": {},
   "source": [
    "- ì›ë³¸ ë°ì´í„°: newsdata/\n",
    "    - â€˜Contentâ€™ ì»¬ëŸ¼: ë‰´ìŠ¤ ë³¸ë¬¸ ì›ë³¸ ê·¸ëŒ€ë¡œ\n",
    "    - â€˜Content_splitâ€™ ì»¬ëŸ¼: â€˜Contentâ€™ ì»¬ëŸ¼ ë°ì´í„°ë¥¼ kss ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë¬¸ì¥ ë¶„ë¦¬í•œ ê²°ê³¼ì…ë‹ˆë‹¤.\n",
    "- ì „ì²˜ë¦¬ ì´í›„ ë°ì´í„°: preprocessed_data/\n",
    "    - â€˜Sentâ€™ ì»¬ëŸ¼ì´ ì „ì²˜ë¦¬ ì´í›„ ê¸°ì‚¬ ë³¸ë¬¸ì´ê³ , ë‚˜ë¨¸ì§€ëŠ” ê¸°ì‚¬ ì œëª©, ë­í‚¹, ì¡°íšŒìˆ˜ ë“± ë©”íƒ€ë°ì´í„°ì…ë‹ˆë‹¤.\n",
    "    - ì–¸ë¡ ì‚¬ë³„ ìƒìœ„ 20ê°œì”© ì •ë ¬ë˜ì–´ìˆìŠµë‹ˆë‹¤. (ex. MBC 1-20ìœ„ ê¸°ì‚¬ -> KBS 1-20ìœ„ ê¸°ì‚¬ -> SBS 1-20ìœ„ ê¸°ì‚¬)\n",
    "    - ë„‰ë„‰í•˜ê²Œ 8/1 ~ 10/13 ê¸°ê°„ ë™ì•ˆ ë°©ì†¡ 3ì‚¬ì˜ ìƒìœ„ 20ê°œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. ì´ 74ê°œì˜ íŒŒì¼, (31+30+13)x20x3=4440ê°œì˜ ê¸°ì‚¬ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### kss: ë¬¸ì¥ ë¶„ë¦¬ì— ì‚¬ìš©\n",
    "### newspaper3k: ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§ì— ì‚¬ìš©\n",
    "\n",
    "# !pip install kss\n",
    "# !pip install newspaper3k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36b45e47",
   "metadata": {},
   "source": [
    "## 1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ - ì–¸ë¡ ì‚¬ë³„\n",
    "\n",
    "- ë„¤ì´ë²„ ë‰´ìŠ¤ ì–¸ë¡ ì‚¬ë³„ ë‰´ìŠ¤\n",
    "- ë§¤ì¼ ì–¸ë¡ ì‚¬ë³„ ë­í‚¹ ìƒìœ„ 20ê°œì”© ìˆ˜ì§‘(MBC, KBS, SBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4acc88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import kss\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "press_id = {\"MBC\":\"214\", \"KBS\":\"056\", \"SBS\":\"055\"}\n",
    "# press_id = {\"MBC\":\"214\", \"KBS\":\"056\", \"SBS\":\"055\", \"JTBC\":\"437\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking_news(save_dir, date, n=1):\n",
    "    total_time = 0\n",
    "    news_list = []\n",
    "    \n",
    "    \n",
    "    #ì–¸ë¡ ì‚¬\n",
    "    for press in press_id:\n",
    "        start = time.time()\n",
    "        url = \"https://news.naver.com/main/ranking/office.nhn?officeId=\" + press_id[press] + \"&date=\" + str(date)\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0(Windows NT 10.0;Win64;x64)AppleWebKit/537.36(KHTML, like Gecko)Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        \n",
    "        ranking_box = soup.find_all(class_=\"rankingnews_box_inner\")\n",
    "        \n",
    "        \n",
    "        #ì¡°íšŒìˆ˜(0) -> ëŒ“ê¸€ìˆ˜(1)\n",
    "        for ranking_type in range(1):\n",
    "            ranking = ranking_box[ranking_type].find_all(class_=\"list_ranking_num\")\n",
    "            url_list = ranking_box[ranking_type].find_all(class_=\"list_content\")\n",
    "            \n",
    "            # ë­í‚¹ìˆœ ìƒìœ„ nê°œ\n",
    "            for rank in range(n):\n",
    "                d = {}\n",
    "                d['Date'] = int(date)\n",
    "                d['Press'] = press\n",
    "                d['Rank'] = ranking[rank].get_text()\n",
    "                d['URL'] = url_list[rank].find('a')['href']\n",
    "                d['Title'] = url_list[rank].find('a').get_text()\n",
    "                if(ranking_type == 0):\n",
    "                    d['View'] = url_list[rank].find(class_=\"list_view\").get_text()\n",
    "                elif(ranking_type==1):\n",
    "                    d['Comment'] = url_list[rank].find(class_=\"list_comment nclicks('RBP.dcmtnwscmt')\").get_text()\n",
    "                news_list.append(d)\n",
    "        \n",
    "        # ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "        for news in news_list:\n",
    "            resp = requests.get(\"https://news.naver.com\" + news['URL'], headers=headers)\n",
    "            news_url = \"https://news.naver.com\" + news['URL']\n",
    "            article = Article(news_url, language='ko')\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            contents = article.text\n",
    "            contents_split = article.text.split('\\n')\n",
    "            \n",
    "            news['Content'] = contents\n",
    "            news['Content_split'] = contents_split\n",
    "#             print('contents:', contents)\n",
    "#             print('contents_split:', contents_split)\n",
    "#             print(len(contents_split))\n",
    "\n",
    "    news_data = pd.DataFrame(news_list)   \n",
    "    save_path = os.path.join(save_dir, 'ranking_' + str(date) + '.json')\n",
    "    news_data.to_json(save_path, orient='table', index=False, force_ascii=False, indent=4)\n",
    "    \n",
    "\n",
    "    end=time.time()\n",
    "    total_time += end - start\n",
    "    print(\"Saved:\" + save_path)\n",
    "    print(\"Total time:\", total_time)\n",
    "    print(\"Average time\", total_time/len(press_id))\n",
    "    print(\"--------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2cf53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 10ì›” ìƒìœ„ 20ê°œ ë‰´ìŠ¤\n",
    "# days = 13\n",
    "# topk = 20\n",
    "# date = 20221001\n",
    "\n",
    "\n",
    "# # 9ì›” ìƒìœ„ 20ê°œ ë‰´ìŠ¤\n",
    "# days = 30\n",
    "# topk = 20\n",
    "# date = 20220901\n",
    "\n",
    "\n",
    "# 8ì›” ìƒìœ„ 20ê°œ ë‰´ìŠ¤\n",
    "days = 31\n",
    "topk = 20\n",
    "date = 20220801\n",
    "\n",
    "\n",
    "save_dir = '/workspace/chitchat_cb/newsdata'\n",
    "\n",
    "for _ in range(days):\n",
    "    raw_data = get_ranking_news(save_dir, date, n=topk)\n",
    "    date += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85394d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e42429a6",
   "metadata": {},
   "source": [
    "## 2. ì „ì²˜ë¦¬\n",
    "- [ì°¸ê³  ì½”ë“œ](https://github.com/HanNayeoniee/boostcamp/blob/main/week10-KLUE/(2%EA%B0%95)%20%EC%9E%90%EC%97%B0%EC%96%B4%EC%9D%98%20%EC%A0%84%EC%B2%98%EB%A6%AC%20-%200_%ED%95%9C%EA%B5%AD%EC%96%B4%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb)\n",
    "- private repoì´ë¯€ë¡œ ê¶Œí•œ ìš”ì²­í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2306494",
   "metadata": {},
   "outputs": [],
   "source": [
    "### kss ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•´ ë¬¸ì¥ ë¶„ë¦¬\n",
    "def split_sentence(data):\n",
    "    sents = []\n",
    "    for sent in data:\n",
    "        split_sent = kss.split_sentences(sent)\n",
    "        sents.extend(split_sent)\n",
    "\n",
    "    return sents\n",
    "\n",
    "\n",
    "def remove_pattern(texts):\n",
    "    outs = []\n",
    "    p1 = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')  # ì´ë©”ì¼(ì˜ì–´@ì˜ì–´)\n",
    "    p2 = re.compile(r'@[\\w\\.-]+')  # (@ì˜ì–´)\n",
    "    p3 = re.compile('\\d{2,3}-\\d{3,4}-\\d{4}$')  # ì¼ë°˜ ì „í™”ë²ˆí˜¸\n",
    "    p4 = re.compile('\\d{3}-\\d{3,4}-\\d{4}$')  # íœ´ëŒ€í°ë²ˆí˜¸\n",
    "    p5 = re.compile('^â—€')\n",
    "    p6 = re.compile('^â–·')\n",
    "    p7 = re.compile('^=')\n",
    "    p8 = re.compile('^MBCë‰´ìŠ¤')\n",
    "    p9 = re.compile('^ì˜ìƒì œê³µ : ')\n",
    "    # kbs\n",
    "    p10 = re.compile('^KBS ë‰´ìŠ¤')\n",
    "    p11 = re.compile('^ì˜ìƒí¸ì§‘:')\n",
    "    p12 = re.compile('^ì´¬ì˜ê¸°ì:')\n",
    "    p13 = re.compile('^\\(ì˜ìƒì·¨ì¬ :')\n",
    "    \n",
    "\n",
    "    for text in texts:    \n",
    "        res1 = p1.findall(text)\n",
    "        res2 = p2.findall(text)\n",
    "        res3 = p3.findall(text)\n",
    "        res4 = p4.findall(text)\n",
    "        res5 = p5.findall(text)\n",
    "        res6 = p6.findall(text)\n",
    "        res7 = p7.findall(text)\n",
    "        res8 = p8.findall(text)\n",
    "        res9 = p9.findall(text)\n",
    "        res10 = p10.findall(text)\n",
    "        res11 = p11.findall(text)\n",
    "        res12 = p12.findall(text)\n",
    "        res13 = p13.findall(text)\n",
    "        \n",
    "        if not res1 and not res2 and not res3 and not res4 and not res5 and not res6 and not res7 and not res8 \\\n",
    "            and not res9 and not res10 and not res11 and not res12 and not res13:\n",
    "            outs.append(text)\n",
    "\n",
    "    return outs\n",
    "\n",
    "\n",
    "def remove_stops(texts):\n",
    "    outs = []\n",
    "    stop_mbc = ['MBC ë‰´ìŠ¤ëŠ” 24ì‹œê°„ ì—¬ëŸ¬ë¶„ì˜ ì œë³´ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.', '[ë‰´ìŠ¤íˆ¬ë°ì´]', '[íƒì‚¬ê¸°íš ìŠ¤íŠ¸ë ˆì´íŠ¸]']\n",
    "    \n",
    "    for text in texts:    \n",
    "        if text not in stop_mbc:\n",
    "            outs.append(text)\n",
    "\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25ddf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_press(texts):\n",
    "    patterns = [r\"\\(ì‚¬ì§„=[ê°€-í£]{3,4}\\)$\",  # (ì‚¬ì§„=ì—°í•©ë‰´ìŠ¤)\n",
    "                    r\"\\(ì‚¬ì§„=[ê°€-í£]{3,4} [ê°€-í£]{3,4}\\)$\",  # (ì‚¬ì§„=ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹°)\n",
    "                    r\"(ì´¬ì˜ê¸°ì:|ì˜ìƒí¸ì§‘:|ê·¸ë˜í”½:|ì˜ìƒì·¨ì¬:|í¸ì§‘:)[ê°€-í£]{2,4}\",  # ì´¬ì˜ê¸°ì:ìœ¤ëŒ€ë¯¼/ì˜ìƒí¸ì§‘:ìµœê·¼í˜/ê·¸ë˜í”½:ê¹€ì„í›ˆ\n",
    "                    r\"(ì´¬ì˜ê¸°ì: |ì˜ìƒí¸ì§‘: |ê·¸ë˜í”½: |ì˜ìƒì·¨ì¬: |í¸ì§‘: )[ê°€-í£]{2,4}\",  # ì´¬ì˜ê¸°ì: ìœ¤ëŒ€ë¯¼/ì˜ìƒí¸ì§‘: ìµœê·¼í˜/ê·¸ë˜í”½: ê¹€ì„í›ˆ\n",
    "                    r\"(ì˜ìƒì·¨ì¬|ì˜ìƒí¸ì§‘|ê·¸ë˜í”½|ì˜ìƒì·¨ì¬|í¸ì§‘)Â·(ì˜ìƒì·¨ì¬|ì˜ìƒí¸ì§‘|ê·¸ë˜í”½|ì˜ìƒì·¨ì¬|í¸ì§‘): [ê°€-í£]{2,4}\",  # ì˜ìƒì·¨ì¬Â·í¸ì§‘: ìœ„ë™ì› \n",
    "                    r\"(ì˜ìƒì·¨ì¬|ì˜ìƒí¸ì§‘|ê·¸ë˜í”½|ì˜ìƒì·¨ì¬|í¸ì§‘)Â·(ì˜ìƒì·¨ì¬|ì˜ìƒí¸ì§‘|ê·¸ë˜í”½|ì˜ìƒì·¨ì¬|í¸ì§‘):[ê°€-í£]{2,4}\",  # ì˜ìƒì·¨ì¬Â·í¸ì§‘:ìœ„ë™ì› \n",
    "                    r\"\\((ì˜ìƒì·¨ì¬ : |ì˜ìƒí¸ì§‘ : |ê·¸ë˜í”½ : |í¸ì§‘: )[ê°€-í£]{2,4}Â·[ê°€-í£]{2,4}\", # (ì˜ìƒì·¨ì¬ : ê¹€ê· ì¢…Â·ì¡°ì°½í˜„\n",
    "                    r\"(ì˜ìƒì·¨ì¬ : |ì˜ìƒí¸ì§‘ : |ê·¸ë˜í”½ : |í¸ì§‘ : \\))[ê°€-í£]{2,4}\",  # ì˜ìƒí¸ì§‘ : ë°•ì§€ì¸\n",
    "                    r\"\\((ì˜ìƒì·¨ì¬ : |ì˜ìƒí¸ì§‘ : |ê·¸ë˜í”½ : |í¸ì§‘ : \\))[ê°€-í£]{2,4}\",  # (ì˜ìƒí¸ì§‘ : ë°•ì§€ì¸\n",
    "                    r\"\\[íƒì‚¬ê¸°íš ìŠ¤íŠ¸ë ˆì´íŠ¸]\",\n",
    "                    r\"\\[ì„¤ë¬¸ ì°¸ì—¬í•˜ê¸°]\",\n",
    "                    r\"<ê¸°ì>\",\n",
    "                    r\"<ì•µì»¤>\",\n",
    "                    r\"ì•µì»¤>\"\n",
    "                    ]\n",
    "    \n",
    "    outs = []\n",
    "    for text in texts:\n",
    "        for pat in patterns:\n",
    "            text = re.sub(pat, \"\", text).strip()\n",
    "        if text:\n",
    "            outs.append(text)    \n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9306ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(texts):\n",
    "    \"\"\"\n",
    "    URLì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    ``ì£¼ì†Œ: www.naver.com`` -> ``ì£¼ì†Œ: ``\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    for text in texts:\n",
    "        text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text).strip()\n",
    "        text = re.sub(r\"pic\\.(\\w+\\.)+\\S*\", \"\", text).strip()\n",
    "        if text:\n",
    "            outs.append(text)\n",
    "    return outs\n",
    "\n",
    "\n",
    "def filter(texts):\n",
    "    outs = []\n",
    "    for text in texts:\n",
    "        text = re.sub('[â–²â”â– â–¶â—€â–³â˜â– â–²â€»ğŸ§]', '', text)\n",
    "        if text:\n",
    "            outs.append(text)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e39aae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsoníŒŒì¼ ê°œìˆ˜: 3\n",
      "ë¬¸ì¥ ê°œìˆ˜: 9\n",
      "ë¬¸ì¥ ê°œìˆ˜: 43\n",
      "ë¬¸ì¥ ê°œìˆ˜: 8\n",
      "ë¬¸ì¥ ê°œìˆ˜: 7\n",
      "ë¬¸ì¥ ê°œìˆ˜: 3\n",
      "ë¬¸ì¥ ê°œìˆ˜: 4\n",
      "ë¬¸ì¥ ê°œìˆ˜: 10\n",
      "ë¬¸ì¥ ê°œìˆ˜: 12\n",
      "ë¬¸ì¥ ê°œìˆ˜: 10\n",
      "ë¬¸ì¥ ê°œìˆ˜: 23\n",
      "ë¬¸ì¥ ê°œìˆ˜: 16\n",
      "ë¬¸ì¥ ê°œìˆ˜: 22\n",
      "ë¬¸ì¥ ê°œìˆ˜: 15\n",
      "ë¬¸ì¥ ê°œìˆ˜: 12\n",
      "ë¬¸ì¥ ê°œìˆ˜: 12\n",
      "ë¬¸ì¥ ê°œìˆ˜: 3\n",
      "ë¬¸ì¥ ê°œìˆ˜: 16\n",
      "ë¬¸ì¥ ê°œìˆ˜: 4\n",
      "ë¬¸ì¥ ê°œìˆ˜: 15\n",
      "ë¬¸ì¥ ê°œìˆ˜: 20\n",
      "Save to: ./test2/20220802.json\n",
      "ë¬¸ì¥ ê°œìˆ˜: 9\n",
      "ë¬¸ì¥ ê°œìˆ˜: 8\n",
      "ë¬¸ì¥ ê°œìˆ˜: 51\n",
      "ë¬¸ì¥ ê°œìˆ˜: 10\n",
      "ë¬¸ì¥ ê°œìˆ˜: 3\n",
      "ë¬¸ì¥ ê°œìˆ˜: 10\n",
      "ë¬¸ì¥ ê°œìˆ˜: 14\n",
      "ë¬¸ì¥ ê°œìˆ˜: 11\n",
      "ë¬¸ì¥ ê°œìˆ˜: 9\n",
      "ë¬¸ì¥ ê°œìˆ˜: 4\n",
      "ë¬¸ì¥ ê°œìˆ˜: 10\n",
      "ë¬¸ì¥ ê°œìˆ˜: 11\n",
      "ë¬¸ì¥ ê°œìˆ˜: 17\n",
      "ë¬¸ì¥ ê°œìˆ˜: 8\n",
      "ë¬¸ì¥ ê°œìˆ˜: 9\n",
      "ë¬¸ì¥ ê°œìˆ˜: 8\n",
      "ë¬¸ì¥ ê°œìˆ˜: 13\n",
      "ë¬¸ì¥ ê°œìˆ˜: 8\n",
      "ë¬¸ì¥ ê°œìˆ˜: 17\n",
      "ë¬¸ì¥ ê°œìˆ˜: 24\n",
      "Save to: ./test2/20220803.json\n",
      "ë¬¸ì¥ ê°œìˆ˜: 2\n",
      "ë¬¸ì¥ ê°œìˆ˜: 10\n",
      "ë¬¸ì¥ ê°œìˆ˜: 3\n",
      "ë¬¸ì¥ ê°œìˆ˜: 3\n",
      "ë¬¸ì¥ ê°œìˆ˜: 6\n",
      "ë¬¸ì¥ ê°œìˆ˜: 21\n",
      "ë¬¸ì¥ ê°œìˆ˜: 4\n",
      "ë¬¸ì¥ ê°œìˆ˜: 3\n",
      "ë¬¸ì¥ ê°œìˆ˜: 8\n",
      "ë¬¸ì¥ ê°œìˆ˜: 9\n",
      "ë¬¸ì¥ ê°œìˆ˜: 33\n",
      "ë¬¸ì¥ ê°œìˆ˜: 23\n",
      "ë¬¸ì¥ ê°œìˆ˜: 21\n",
      "ë¬¸ì¥ ê°œìˆ˜: 8\n",
      "ë¬¸ì¥ ê°œìˆ˜: 4\n",
      "ë¬¸ì¥ ê°œìˆ˜: 3\n",
      "ë¬¸ì¥ ê°œìˆ˜: 4\n",
      "Save to: ./test2/20220801.json\n"
     ]
    }
   ],
   "source": [
    "## ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "## ì—¬ê¸° ê¸°ì¡´ ì½”ë“œ ë‚ ì•„ê°”ìŒ\n",
    "\n",
    "# target = './newsdata/*.json'\n",
    "target = './test/*.json'\n",
    "json_list = glob.glob(target)\n",
    "print('jsoníŒŒì¼ ê°œìˆ˜:', len(json_list))\n",
    "\n",
    "\n",
    "for file in json_list:\n",
    "    with open(file, 'r') as f:\n",
    "        json_data = json.load(f)[\"data\"]\n",
    "        df = pd.DataFrame(json_data)\n",
    "\n",
    "\n",
    "        sents = []\n",
    "        for data in df[\"content\"]:\n",
    "            split_sent = split_sentence(data)\n",
    "            print('ë¬¸ì¥ ê°œìˆ˜:', len(split_sent))\n",
    "            outs = remove_pattern(split_sent)\n",
    "            outs = remove_stops(outs)\n",
    "            outs = remove_press(outs)\n",
    "            outs = remove_url(outs)\n",
    "            outs = filter(outs)        \n",
    "            sents.append(split_sent)\n",
    "\n",
    "\n",
    "#     df['sent'] = sents\n",
    "    df['sent'] = sents\n",
    "#     df = df[['Date', 'Press', 'Rank', 'URL', 'Title', 'View', 'Sent']]\n",
    "    save_path = os.path.join('./test2/', Path(file).stem + '.json')\n",
    "    df.to_json(save_path, orient='table', index=False, force_ascii=False, indent=4)\n",
    "    print('Save to:', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fac5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c5796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e12d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [\"ì„¸ê³„ ìµœì´ˆ ë²„ì¸„ì–¼ íœ´ë¨¼ ê±¸ê·¸ë£¹ì¸ ì´í„°ë‹ˆí‹°ê°€ ì‚¬ìƒ ì²˜ìŒìœ¼ë¡œ TV ìƒë°©ì†¡ì— ì¶œì—°í–ˆìŠµë‹ˆë‹¤.\", \"dkssud\"]\n",
    "split_sentence(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e35e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
