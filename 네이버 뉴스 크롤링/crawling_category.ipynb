{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8c10a50",
   "metadata": {},
   "source": [
    "- ì›ë³¸ ë°ì´í„°: newsdata/\n",
    "    - â€˜Contentâ€™ ì»¬ëŸ¼: ë‰´ìŠ¤ ë³¸ë¬¸ ì›ë³¸ ê·¸ëŒ€ë¡œ\n",
    "    - â€˜Content_splitâ€™ ì»¬ëŸ¼: â€˜Contentâ€™ ì»¬ëŸ¼ ë°ì´í„°ë¥¼ kss ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë¬¸ì¥ ë¶„ë¦¬í•œ ê²°ê³¼ì…ë‹ˆë‹¤.\n",
    "- ì „ì²˜ë¦¬ ì´í›„ ë°ì´í„°: preprocessed_data/\n",
    "    - â€˜Sentâ€™ ì»¬ëŸ¼ì´ ì „ì²˜ë¦¬ ì´í›„ ê¸°ì‚¬ ë³¸ë¬¸ì´ê³ , ë‚˜ë¨¸ì§€ëŠ” ê¸°ì‚¬ ì œëª©, ë­í‚¹, ì¡°íšŒìˆ˜ ë“± ë©”íƒ€ë°ì´í„°ì…ë‹ˆë‹¤.\n",
    "    - ì–¸ë¡ ì‚¬ë³„ ìƒìœ„ 20ê°œì”© ì •ë ¬ë˜ì–´ìˆìŠµë‹ˆë‹¤. (ex. MBC 1-20ìœ„ ê¸°ì‚¬ -> KBS 1-20ìœ„ ê¸°ì‚¬ -> SBS 1-20ìœ„ ê¸°ì‚¬)\n",
    "    - ë„‰ë„‰í•˜ê²Œ 8/1 ~ 10/13 ê¸°ê°„ ë™ì•ˆ ë°©ì†¡ 3ì‚¬ì˜ ìƒìœ„ 20ê°œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. ì´ 74ê°œì˜ íŒŒì¼, (31+30+13)x20x3=4440ê°œì˜ ê¸°ì‚¬ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### kss: ë¬¸ì¥ ë¶„ë¦¬ì— ì‚¬ìš©\n",
    "### newspaper3k: ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§ì— ì‚¬ìš©\n",
    "\n",
    "# !pip install kss\n",
    "# !pip install newspaper3k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36b45e47",
   "metadata": {},
   "source": [
    "## 1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ - ê¸°ì‚¬ ë¶„ì•¼ë³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85394d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import kss\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "def get_href(soup):\n",
    "    # ê° ë¶„ì•¼ë³„ ì†ë³´ ê¸°ì‚¬ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” hrefë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    div = soup.find(\"div\", class_=\"list_body newsflash_body\")\n",
    "    \n",
    "    for dt in div.find_all(\"dt\", class_=\"photo\"):\n",
    "        result.append(dt.find(\"a\")[\"href\"])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881129e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_href_daily(section, date):\n",
    "    custom_header = {\n",
    "        'referer' : 'https://www.naver.com/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "    }\n",
    "    # https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=245&sid1=103&date=20220801\n",
    "    url = \"https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=245&sid1=\" + str(section) + \"&date=\" + str(date)\n",
    "    req = requests.get(url, headers=custom_header)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    list_href = get_href(soup)\n",
    "    \n",
    "    return list_href\n",
    "\n",
    "\n",
    "def crawl_daily(news_list, date, save_path):\n",
    "    dates = [str(date)] * len(news_list)\n",
    "    ids = [str(i) for i in range(len(news_list))]\n",
    "\n",
    "    titles = []\n",
    "    contents = []\n",
    "    for news_url in news_list:\n",
    "        article = Article(news_url, language='ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        title = article.title\n",
    "        content = article.text.split('\\n')\n",
    "        titles.append(title)\n",
    "        contents.append(content)\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame({'date': dates,\n",
    "                       'id': ids,\n",
    "                       'title': titles,\n",
    "                       'content': contents,\n",
    "                       'url': news_list\n",
    "                      })\n",
    "    \n",
    "    out_path = os.path.join(save_path, str(date) + '.json')\n",
    "    df.to_json(out_path, orient='table', index=False, force_ascii=False, indent=4)\n",
    "    print('Saved to: ', out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a139569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221001.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221002.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221003.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221004.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221005.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221006.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221007.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221008.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221009.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221010.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221011.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221012.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221013.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221014.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221015.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221016.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221017.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221018.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221019.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221020.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221021.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221022.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221023.json\n",
      "Saved to:  /workspace/chitchat_cb/newsdata_culture/20221024.json\n"
     ]
    }
   ],
   "source": [
    "# {'ì •ì¹˜': 100, 'ê²½ì œ': 101, 'ì‚¬íšŒ': 102, 'ìƒí™œ/ë¬¸í™”': 103, 'ì„¸ê³„': 104, 'IT/ê³¼í•™': 105}\n",
    "\n",
    "# test\n",
    "# section = 103\n",
    "# date = 20220801\n",
    "# days = 3\n",
    "# save_path = '/workspace/chitchat_cb/test'\n",
    "\n",
    "\n",
    "section = 103\n",
    "\n",
    "### 8ì›”\n",
    "# date = 20220801\n",
    "# days = 31\n",
    "\n",
    "### 9ì›”\n",
    "# date = 20220901\n",
    "# days = 30\n",
    "\n",
    "### 10ì›”\n",
    "date = 20221001\n",
    "days = 24\n",
    "\n",
    "\n",
    "save_path = '/workspace/chitchat_cb/newsdata_culture'\n",
    "\n",
    "for _ in range(days):\n",
    "    news_list = get_href_daily(section, date)\n",
    "    crawl_daily(news_list, date, save_path)\n",
    "    date += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af27fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde99870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e42429a6",
   "metadata": {},
   "source": [
    "## 2. ì „ì²˜ë¦¬\n",
    "- [ì°¸ê³  ì½”ë“œ](https://github.com/HanNayeoniee/boostcamp/blob/main/week10-KLUE/(2%EA%B0%95)%20%EC%9E%90%EC%97%B0%EC%96%B4%EC%9D%98%20%EC%A0%84%EC%B2%98%EB%A6%AC%20-%200_%ED%95%9C%EA%B5%AD%EC%96%B4%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb)\n",
    "- private repoì´ë¯€ë¡œ ê¶Œí•œ ìš”ì²­í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2306494",
   "metadata": {},
   "outputs": [],
   "source": [
    "### kss ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•´ ë¬¸ì¥ ë¶„ë¦¬\n",
    "def split_sentence(data):\n",
    "    sents = []\n",
    "    for sent in data:\n",
    "        split_sent = kss.split_sentences(sent)\n",
    "        sents.extend(split_sent)\n",
    "\n",
    "    return sents\n",
    "\n",
    "\n",
    "def remove_pattern(texts):\n",
    "    outs = []\n",
    "    p1 = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')  # ì´ë©”ì¼(ì˜ì–´@ì˜ì–´)\n",
    "    p2 = re.compile(r'@[\\w\\.-]+')  # (@ì˜ì–´)\n",
    "    p3 = re.compile('\\d{2,3}-\\d{3,4}-\\d{4}$')  # ì¼ë°˜ ì „í™”ë²ˆí˜¸\n",
    "    p4 = re.compile('\\d{3}-\\d{3,4}-\\d{4}$')  # íœ´ëŒ€í°ë²ˆí˜¸\n",
    "    p5 = re.compile('^â—€')\n",
    "    p6 = re.compile('^â–·')\n",
    "    p7 = re.compile('^=')\n",
    "    p8 = re.compile('^MBCë‰´ìŠ¤')\n",
    "    p9 = re.compile('^ì˜ìƒì œê³µ : ')\n",
    "    # kbs\n",
    "    p10 = re.compile('^KBS ë‰´ìŠ¤')\n",
    "    p11 = re.compile('^ì˜ìƒí¸ì§‘:')\n",
    "    p12 = re.compile('^ì´¬ì˜ê¸°ì:')\n",
    "    p13 = re.compile('^\\(ì˜ìƒì·¨ì¬ :')\n",
    "    \n",
    "\n",
    "    for text in texts:    \n",
    "        res1 = p1.findall(text)\n",
    "        res2 = p2.findall(text)\n",
    "        res3 = p3.findall(text)\n",
    "        res4 = p4.findall(text)\n",
    "        res5 = p5.findall(text)\n",
    "        res6 = p6.findall(text)\n",
    "        res7 = p7.findall(text)\n",
    "        res8 = p8.findall(text)\n",
    "        res9 = p9.findall(text)\n",
    "        res10 = p10.findall(text)\n",
    "        res11 = p11.findall(text)\n",
    "        res12 = p12.findall(text)\n",
    "        res13 = p13.findall(text)\n",
    "        \n",
    "        if not res1 and not res2 and not res3 and not res4 and not res5 and not res6 and not res7 and not res8 \\\n",
    "            and not res9 and not res10 and not res11 and not res12 and not res13:\n",
    "            outs.append(text)\n",
    "\n",
    "    return outs\n",
    "\n",
    "\n",
    "def remove_stops(texts):\n",
    "    outs = []\n",
    "    stop_mbc = ['MBC ë‰´ìŠ¤ëŠ” 24ì‹œê°„ ì—¬ëŸ¬ë¶„ì˜ ì œë³´ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.', '[ë‰´ìŠ¤íˆ¬ë°ì´]', '[íƒì‚¬ê¸°íš ìŠ¤íŠ¸ë ˆì´íŠ¸]']\n",
    "    \n",
    "    for text in texts:    \n",
    "        if text not in stop_mbc:\n",
    "            outs.append(text)\n",
    "\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e25ddf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_press(texts):\n",
    "    patterns = [r\"\\(ì‚¬ì§„=[ê°€-í£]{3,4}\\)$\",  # (ì‚¬ì§„=ì—°í•©ë‰´ìŠ¤)\n",
    "                    r\"\\(ì‚¬ì§„=[ê°€-í£]{3,4} [ê°€-í£]{3,4}\\)$\",  # (ì‚¬ì§„=ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹°)\n",
    "                    r\"(ì´¬ì˜ê¸°ì:|ì˜ìƒí¸ì§‘:|ê·¸ë˜í”½:|ì˜ìƒì·¨ì¬:|í¸ì§‘:)[ê°€-í£]{2,4}\",  # ì´¬ì˜ê¸°ì:ìœ¤ëŒ€ë¯¼/ì˜ìƒí¸ì§‘:ìµœê·¼í˜/ê·¸ë˜í”½:ê¹€ì„í›ˆ\n",
    "                    r\"(ì´¬ì˜ê¸°ì: |ì˜ìƒí¸ì§‘: |ê·¸ë˜í”½: |ì˜ìƒì·¨ì¬: |í¸ì§‘: )[ê°€-í£]{2,4}\",  # ì´¬ì˜ê¸°ì: ìœ¤ëŒ€ë¯¼/ì˜ìƒí¸ì§‘: ìµœê·¼í˜/ê·¸ë˜í”½: ê¹€ì„í›ˆ\n",
    "                    r\"(ì˜ìƒì·¨ì¬|ì˜ìƒí¸ì§‘|ê·¸ë˜í”½|ì˜ìƒì·¨ì¬|í¸ì§‘)Â·(ì˜ìƒì·¨ì¬|ì˜ìƒí¸ì§‘|ê·¸ë˜í”½|ì˜ìƒì·¨ì¬|í¸ì§‘): [ê°€-í£]{2,4}\",  # ì˜ìƒì·¨ì¬Â·í¸ì§‘: ìœ„ë™ì› \n",
    "                    r\"(ì˜ìƒì·¨ì¬|ì˜ìƒí¸ì§‘|ê·¸ë˜í”½|ì˜ìƒì·¨ì¬|í¸ì§‘)Â·(ì˜ìƒì·¨ì¬|ì˜ìƒí¸ì§‘|ê·¸ë˜í”½|ì˜ìƒì·¨ì¬|í¸ì§‘):[ê°€-í£]{2,4}\",  # ì˜ìƒì·¨ì¬Â·í¸ì§‘:ìœ„ë™ì› \n",
    "                    r\"\\((ì˜ìƒì·¨ì¬ : |ì˜ìƒí¸ì§‘ : |ê·¸ë˜í”½ : |í¸ì§‘: )[ê°€-í£]{2,4}Â·[ê°€-í£]{2,4}\", # (ì˜ìƒì·¨ì¬ : ê¹€ê· ì¢…Â·ì¡°ì°½í˜„\n",
    "                    r\"(ì˜ìƒì·¨ì¬ : |ì˜ìƒí¸ì§‘ : |ê·¸ë˜í”½ : |í¸ì§‘ : \\))[ê°€-í£]{2,4}\",  # ì˜ìƒí¸ì§‘ : ë°•ì§€ì¸\n",
    "                    r\"\\((ì˜ìƒì·¨ì¬ : |ì˜ìƒí¸ì§‘ : |ê·¸ë˜í”½ : |í¸ì§‘ : \\))[ê°€-í£]{2,4}\",  # (ì˜ìƒí¸ì§‘ : ë°•ì§€ì¸\n",
    "                    r\"\\[íƒì‚¬ê¸°íš ìŠ¤íŠ¸ë ˆì´íŠ¸]\",\n",
    "                    r\"\\[ì„¤ë¬¸ ì°¸ì—¬í•˜ê¸°]\",\n",
    "                    r\"<ê¸°ì>\",\n",
    "                    r\"<ì•µì»¤>\",\n",
    "                    r\"ì•µì»¤>\"\n",
    "                    ]\n",
    "    \n",
    "    outs = []\n",
    "    for text in texts:\n",
    "        for pat in patterns:\n",
    "            text = re.sub(pat, \"\", text).strip()\n",
    "        if text:\n",
    "            outs.append(text)    \n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9306ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(texts):\n",
    "    \"\"\"\n",
    "    URLì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    ``ì£¼ì†Œ: www.naver.com`` -> ``ì£¼ì†Œ: ``\n",
    "    \"\"\"\n",
    "    outs = []\n",
    "    for text in texts:\n",
    "        text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text).strip()\n",
    "        text = re.sub(r\"pic\\.(\\w+\\.)+\\S*\", \"\", text).strip()\n",
    "        if text:\n",
    "            outs.append(text)\n",
    "    return outs\n",
    "\n",
    "\n",
    "def filter(texts):\n",
    "    outs = []\n",
    "    for text in texts:\n",
    "        text = re.sub('[â–²â”â– â–¶â—€â–³â˜â– â–²â€»ğŸ§]', '', text)\n",
    "        if text:\n",
    "            outs.append(text)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e39aae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsoníŒŒì¼ ê°œìˆ˜: 85\n",
      "Save to: ./newsdata_culture_res/20220815.json\n",
      "Save to: ./newsdata_culture_res/20220822.json\n",
      "Save to: ./newsdata_culture_res/20221012.json\n",
      "Save to: ./newsdata_culture_res/20220804.json\n",
      "Save to: ./newsdata_culture_res/20221008.json\n",
      "Save to: ./newsdata_culture_res/20220910.json\n",
      "Save to: ./newsdata_culture_res/20220823.json\n",
      "Save to: ./newsdata_culture_res/20220814.json\n",
      "Save to: ./newsdata_culture_res/20220912.json\n",
      "Save to: ./newsdata_culture_res/20221010.json\n",
      "Save to: ./newsdata_culture_res/20220830.json\n",
      "Save to: ./newsdata_culture_res/20220827.json\n",
      "Save to: ./newsdata_culture_res/20220922.json\n",
      "Save to: ./newsdata_culture_res/20221024.json\n",
      "Save to: ./newsdata_culture_res/20220802.json\n",
      "Save to: ./newsdata_culture_res/20220909.json\n",
      "Save to: ./newsdata_culture_res/20220803.json\n",
      "Save to: ./newsdata_culture_res/20220923.json\n",
      "Save to: ./newsdata_culture_res/20220806.json\n",
      "Save to: ./newsdata_culture_res/20220927.json\n",
      "Save to: ./newsdata_culture_res/20220810.json\n",
      "Save to: ./newsdata_culture_res/20221021.json\n",
      "Save to: ./newsdata_culture_res/20220920.json\n",
      "Save to: ./newsdata_culture_res/20221002.json\n",
      "Save to: ./newsdata_culture_res/20220824.json\n",
      "Save to: ./newsdata_culture_res/20221006.json\n",
      "Save to: ./newsdata_culture_res/20220813.json\n",
      "Save to: ./newsdata_culture_res/20220819.json\n",
      "Save to: ./newsdata_culture_res/20220924.json\n",
      "Save to: ./newsdata_culture_res/20221001.json\n",
      "Save to: ./newsdata_culture_res/20221009.json\n",
      "Save to: ./newsdata_culture_res/20221013.json\n",
      "Save to: ./newsdata_culture_res/20220811.json\n",
      "Save to: ./newsdata_culture_res/20220816.json\n",
      "Save to: ./newsdata_culture_res/20220809.json\n",
      "Save to: ./newsdata_culture_res/20220903.json\n",
      "Save to: ./newsdata_culture_res/20221020.json\n",
      "Save to: ./newsdata_culture_res/20220907.json\n",
      "Save to: ./newsdata_culture_res/20220807.json\n",
      "Save to: ./newsdata_culture_res/20220919.json\n",
      "Save to: ./newsdata_culture_res/20220825.json\n",
      "Save to: ./newsdata_culture_res/20221017.json\n",
      "Save to: ./newsdata_culture_res/20221023.json\n",
      "Save to: ./newsdata_culture_res/20220918.json\n",
      "Save to: ./newsdata_culture_res/20221018.json\n",
      "Save to: ./newsdata_culture_res/20220915.json\n",
      "Save to: ./newsdata_culture_res/20221003.json\n",
      "Save to: ./newsdata_culture_res/20220921.json\n",
      "Save to: ./newsdata_culture_res/20220908.json\n",
      "Save to: ./newsdata_culture_res/20221007.json\n",
      "Save to: ./newsdata_culture_res/20221005.json\n",
      "Save to: ./newsdata_culture_res/20220821.json\n",
      "Save to: ./newsdata_culture_res/20221004.json\n",
      "Save to: ./newsdata_culture_res/20220904.json\n",
      "Save to: ./newsdata_culture_res/20220829.json\n",
      "Save to: ./newsdata_culture_res/20220928.json\n",
      "Save to: ./newsdata_culture_res/20220913.json\n",
      "Save to: ./newsdata_culture_res/20221016.json\n",
      "Save to: ./newsdata_culture_res/20220926.json\n",
      "Save to: ./newsdata_culture_res/20220826.json\n",
      "Save to: ./newsdata_culture_res/20220914.json\n",
      "Save to: ./newsdata_culture_res/20220817.json\n",
      "Save to: ./newsdata_culture_res/20220808.json\n",
      "Save to: ./newsdata_culture_res/20221011.json\n",
      "Save to: ./newsdata_culture_res/20221022.json\n",
      "Save to: ./newsdata_culture_res/20220805.json\n",
      "Save to: ./newsdata_culture_res/20220930.json\n",
      "Save to: ./newsdata_culture_res/20220831.json\n",
      "Save to: ./newsdata_culture_res/20221014.json\n",
      "Save to: ./newsdata_culture_res/20220801.json\n",
      "Save to: ./newsdata_culture_res/20220818.json\n",
      "Save to: ./newsdata_culture_res/20220906.json\n",
      "Save to: ./newsdata_culture_res/20220916.json\n",
      "Save to: ./newsdata_culture_res/20220901.json\n",
      "Save to: ./newsdata_culture_res/20220917.json\n",
      "Save to: ./newsdata_culture_res/20220925.json\n",
      "Save to: ./newsdata_culture_res/20220828.json\n",
      "Save to: ./newsdata_culture_res/20220812.json\n",
      "Save to: ./newsdata_culture_res/20220820.json\n",
      "Save to: ./newsdata_culture_res/20220929.json\n",
      "Save to: ./newsdata_culture_res/20220902.json\n",
      "Save to: ./newsdata_culture_res/20221015.json\n",
      "Save to: ./newsdata_culture_res/20220911.json\n",
      "Save to: ./newsdata_culture_res/20220905.json\n",
      "Save to: ./newsdata_culture_res/20221019.json\n"
     ]
    }
   ],
   "source": [
    "## ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "\n",
    "target = './newsdata_culture/*.json'\n",
    "json_list = glob.glob(target)\n",
    "print('jsoníŒŒì¼ ê°œìˆ˜:', len(json_list))\n",
    "\n",
    "\n",
    "for file in json_list:\n",
    "    with open(file, 'r') as f:\n",
    "        json_data = json.load(f)[\"data\"]\n",
    "        df = pd.DataFrame(json_data)\n",
    "\n",
    "\n",
    "        sents = []\n",
    "        for data in df[\"content\"]:\n",
    "            split_sent = split_sentence(data)\n",
    "#             print('ë¬¸ì¥ ê°œìˆ˜:', len(split_sent))\n",
    "            outs = remove_pattern(split_sent)\n",
    "            outs = remove_stops(outs)\n",
    "            outs = remove_press(outs)\n",
    "            outs = remove_url(outs)\n",
    "            outs = filter(outs)        \n",
    "            sents.append(split_sent)\n",
    "\n",
    "    df['sent'] = sents\n",
    "    save_path = os.path.join('./newsdata_culture_res/', Path(file).stem + '.json')\n",
    "    df.to_json(save_path, orient='table', index=False, force_ascii=False, indent=4)\n",
    "    print('Save to:', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fac5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "\n",
    "\n",
    "## ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "\n",
    "target = './newsdata_culture_res/*.json'\n",
    "json_list = glob.glob(target)\n",
    "json_list = natsort.natsorted(json_list)\n",
    "print('jsoníŒŒì¼ ê°œìˆ˜:', len(json_list))\n",
    "\n",
    "\n",
    "start_file = json_list[0]\n",
    "with open(start_file, 'r') as f:\n",
    "    json_data = json.load(f)[\"data\"]\n",
    "    df = pd.DataFrame(json_data)\n",
    "    news_id = [df['date'][i] + '_' + str(df['id'][i]) for i in range(len(df))]\n",
    "    df['news_id'] = news_id\n",
    "    total_df = df[['news_id', 'url', 'title', 'sent']]\n",
    "\n",
    "\n",
    "for file in json_list[1:]:\n",
    "    with open(file, 'r') as f:\n",
    "        json_data = json.load(f)[\"data\"]\n",
    "        df = pd.DataFrame(json_data)\n",
    "        news_id = [df['date'][i] + '_' + str(df['id'][i]) for i in range(len(df))]\n",
    "        df['news_id'] = news_id\n",
    "        df = df[['news_id', 'url', 'title', 'sent']]\n",
    "    total_df = pd.concat([total_df, df])\n",
    "\n",
    "    \n",
    "total_df.to_excel('./final.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bcbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
